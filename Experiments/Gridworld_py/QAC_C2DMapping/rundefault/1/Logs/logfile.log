Module path:  Environments.ToyMaze.Gridworld_py Gridworld_py
Dynamically loaded from:  <class 'Environments.ToyMaze.Gridworld_py.Gridworld_py'>
Movements::  [[ 1.     0.   ]
 [ 0.866  0.5  ]
 [ 0.5    0.866]
 [ 0.     1.   ]
 [-0.5    0.866]
 [-0.866  0.5  ]
 [-1.     0.   ]
 [-0.866 -0.5  ]
 [-0.5   -0.866]
 [-0.    -1.   ]
 [ 0.5   -0.866]
 [ 0.866 -0.5  ]]
Module path:  Src.RL_Algorithms.QAC_C2DMapping QAC_C2DMapping
Dynamically loaded from:  <class 'Src.RL_Algorithms.QAC_C2DMapping.QAC_C2DMapping'>
=====Configurations=====
 Namespace(a_clip=1, acceptanceCooling=0.225, actionLiteral=1, actor_lr=0.01, actor_scaling_factor_mean=1, algo_name='QAC_C2DMapping', batch_size=1, buffer_size=100, clipped_decimals=0, collision_rewards=0, commonOrderCosts=75, constraint='hard', cooling=0, critic_lr=0.01, debug=True, deepActionRep=True, deepActor=False, emb_lambda=0.9, emb_reg=1e-05, embed_lr=0.0001, env_name='Gridworld_py', experiment='run', folder_suffix='default', fourier_coupled=True, fourier_order=3, gamma=0.999, gauss_variance=0.5, hiddenActorLayerSize=128, hiddenLayerSize=32, initialAcceptance=0.9, initial_phase_epochs=4, knns=2, load_embed=False, log_output='term_file', mapping='dnc_mapping', max_episodes=100, max_steps=150, maximum_greedy_search_steps=0, n_actions=12, no_neighbours=0, only_phase_one=False, optim='sgd', perturb_scaler=1, perturbation_range=5, reduced_action_dim=2, save_count=10, save_model=True, seed=1, smax=66, smin=0, state_lr=0.001, sup_batch_size=64, timestamp='5|31|15:45:58', true_embeddings=False, wall=True)
Actions space: 4096 :: State space: 2
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
No action_space_matrix, so mapping returns literal action!
Critic:  [('fc1.weight', torch.Size([32, 16])), ('fc1.bias', torch.Size([32])), ('fc2.weight', torch.Size([32, 44])), ('fc2.bias', torch.Size([32])), ('fc3.weight', torch.Size([1, 32])), ('fc3.bias', torch.Size([1]))]
Actor:  [('fc_mean.weight', torch.Size([12, 16])), ('fc_mean.bias', torch.Size([12]))]
Episode 0 / current actor loss: -0.00447714626789093
Episode 0 / current critic loss: 5.1477616652846335e-05
time required for 10 :0.5691771507263184
